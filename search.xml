<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Attention is All You Need</title>
      <link href="/2021/10/03/attention-is-all-you-need/"/>
      <url>/2021/10/03/attention-is-all-you-need/</url>
      
        <content type="html"><![CDATA[<h1 id="Attention-Is-All-You-Need"><a href="#Attention-Is-All-You-Need" class="headerlink" title="Attention Is All You Need"></a>Attention Is All You Need</h1><h2 id="1-Introduction"><a href="#1-Introduction" class="headerlink" title="1 Introduction"></a>1 Introduction</h2><ul><li>Recurrent neural networks, long short-term memory and gated recurrent neural networks have been state of the art  approaches in sequence modeling and transduction problems such as language modeling and machine translation. 在序列模型(语言建模，机器翻译等)中，RNN(循环神经网络)、LSTM(长短期记忆网络,一种时间循环神经网络)、GRNN(Gated-RNN)一直以来是最优方案。</li><li>Recurrent models typically factor computation along the symbol positions of the input and output sequences. This inherently sequential nature precludes parallelization within training  examples, which becomes critical at longer sequence lengths, as memory  constraints limit batching across examples. 以上模型中，计算隐藏状态$h_t$的函数需要用到$h_{t-1}$(递归),因此无法并行化，严重限制了性能，且难以学习到全局的结构信息。</li><li>Attention mechanisms have become an integral part of compelling sequence  modeling and transduction models in various tasks, allowing modeling of  dependencies without regard to their distance in the input or output sequences. Attention机制不考虑输入或输出序列中的距离，面向依赖性建模。</li><li>In this work we propose the Transformer, a model architecture eschewing  recurrence and instead relying entirely on an attention mechanism to draw global  dependencies between input and output. The Transformer allows for significantly  more parallelization and can reach a new state of the art in translation quality. Transformer完全依赖于注意力机制来获得输入和输出之间的全局依赖关系，允许并行化。</li></ul><h2 id="2-Background"><a href="#2-Background" class="headerlink" title="2 Background"></a>2 Background</h2><ul><li>The number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet. This makes it more difficult to learn dependencies between distant positions. 一些网络如ByteNet, ConvS2S等，使用卷积神经网络作为基本构建块，并行计算所有输入和输出位置的隐藏表示。但距离较远的输入(输出)信号关联起来的计算量随距离的增加而增加，CONVS2为线性，ByteNet为对数。</li><li>In the Transformer this is reduced to a constant number of operations. 而Transformer可以减少到常数级的计算量。</li><li>Self-attention, sometimes called intra-attention is an attention mechanism relating different positions of a single sequence in order to compute a representation of the sequence. 自注意力机制，将单个序列的不同位置联系起来，来计算序列的表示。</li></ul><h2 id="3-Model-Architecture"><a href="#3-Model-Architecture" class="headerlink" title="3 Model Architecture"></a>3 Model Architecture</h2><ul><li>The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder. Transformer使用多层叠加的自注意力层，在编码器和解码器中使用全连接层。</li><li><img src="image-20211027214240983.png" alt="Overall Architecture"></li></ul><h3 id="3-1-Encoder-and-Decoder-stacks"><a href="#3-1-Encoder-and-Decoder-stacks" class="headerlink" title="3.1 Encoder and Decoder stacks"></a>3.1 Encoder and Decoder stacks</h3><p><img src="image-20211027214751365.png" alt="Encoder"></p><ul><li>Encoder<ul><li>The encoder is composed of a stack of N= 6identical layers. 6个相同的层。</li><li>The first is a multi-head self-attention mechanism, and the second is a simple,  position-wise fully connected feed-forward network. 每层包括两个子层。第一个子层是一个多头注意力层，第二个子层是一个简单的位置相关的全连接前馈网络。</li><li>We employ a residual connection around each of the two sub-layers, followed by layer normalization. 每个子层之后，都加一个残差连接块，并进行归一化。</li><li>That is, the output of each sub-layer is $LayerNorm(x+ Sublayer(x))$, where $Sublayer(x)$ is the function implemented by the sub-layer itself.将上一步的内容用公式表达。</li><li>To facilitate these residual connections, all sub-layers in the model, as well  as the embedding layers, produce outputs of dimension $d_{model}$= 512. 所有子层，包括嵌入层的输出维度均为512.</li></ul></li></ul><p><img src="image-20211027220049202.png" alt="Decoder"></p><ul><li>Decoder<ul><li>The decoder is also composed of a stack of N= 6identical layers. 同样是重复六层。</li><li>In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head<br>attention over the output of the encoder stack. Similar to the encoder, we employ residual connections around each of the sub-layers, followed by layer normalization. 与Encoder相比，多了一个多头注意力层，输入为Encoder的输出。同样，每个子层添加残差模块和归一化。</li><li>We also modify the self-attention sub-layer in the decoder stack to prevent  positions from attending to subsequent positions. This masking, combined with  fact that the output embeddings are offset by one position, ensures that the  predictions for position $i$ can depend only on the known outputs at positions less  than $i$. 解码器中的自注意力层加入了mask，来确保对位置$i$的信息预测仅依靠位置$i$之前的信息。这其实是一个合理的常识性问题，对于一个信息序列，对某位置信息的预测应该只考虑该位置之前的信息，因为该位置后面的信息可能并没有。</li></ul></li></ul><h3 id="3-2-Attention"><a href="#3-2-Attention" class="headerlink" title="3.2 Attention"></a>3.2 Attention</h3><h4 id="3-2-1-Scaled-Dot-Product-Attention"><a href="#3-2-1-Scaled-Dot-Product-Attention" class="headerlink" title="3.2.1 Scaled Dot-Product Attention"></a>3.2.1 Scaled Dot-Product Attention</h4><p><img src="image-20211027223544767.png" alt="Scaled Dot-Product Attention"></p><ul><li>The input consists of queries and keys of dimension $d_k$, and values of dimension $d_v$. 输入为三个矩阵$Q,K,V$.</li><li>We compute the dot products of the query with all keys, divide each by $\sqrt{d_k}$, and apply a softmax function to obtain the weights on the values. 每个位置的$Q$矩阵与所有位置的$K$矩阵做点积，除以$\sqrt{d_k}$(为了防止点积过大，$softmax$失去效果，所以需要调节一下)，再接一个$Softmax$层后获得矩阵$V$的权重。</li><li>In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix $Q$. The keys and values are also packed together into matrices $K$ and $V$. 实际计算中，每个位置的矩阵$Q_i$摞在一起形成矩阵$Q$，$K_i$也摞在一起形成$K$，$V$同理。</li><li>$Attention(Q,K,V)=softmax(\frac{QK^T}{\sqrt{d_k}})V, Q\in R^{n \times d_k}, K\in R^{m \times d_k}, V \in R_{m \times d_v}$. 以上内容的公式形式。**可以理解为将 $n×d_k$ 的序列 $Q$ 编码成了一个新的 $n×d_v $的序列$V$**。</li></ul><h4 id="3-2-2-Multi-Head-Attention"><a href="#3-2-2-Multi-Head-Attention" class="headerlink" title="3.2.2 Multi-Head Attention"></a>3.2.2 Multi-Head Attention</h4><ul><li>Instead of performing a single attention function with $d_{model}$-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values $h$ times with different, learned linear projections to $d_k$,$d_k$ and $d_v$ dimensions, respectively. 将$Q,K,V$各自线性投射分成$h$部分，实现多头注意力。</li><li>On each of these projected versions of queries, keys and values we then perform the attention function in parallel, yielding $d_v$-dimensional output values.投射后，$Q,K,V$的维度会减少，并且分出去的几个部分可以并行计算。</li><li>These are concatenated and once again projected, resulting in the final values. 这种投射其实就是把512维的向量拆开，分别计算(多个注意力机制并行计算)后，再连接在一起。</li><li>$MultiHead(Q, K, V) = Concat(head_1,…,head_n)W^o, head_i=Attention(QW_i^Q, KW_i^K, VW_i^V)$.以上内容的公式形式，其中$W_i^Q,W_i^K,W_i^V,W^O$是需要学习的参数。</li><li>In this work we employ $h= 8$ parallel attention layers, or heads. For each of these we use $d_k=d_v=d_{model}/h= 64$. Due to the reduced dimension of each head, the total computational cost is similar to that of single-head attention with full dimensionality. 将此前的$Q,K,V$按维度平均分成8部分，每一部分作为多头注意力的一头。计算量基本与拆分前相同。</li></ul><h4 id="3-2-3-Applications-of-Attention-in-our-Model"><a href="#3-2-3-Applications-of-Attention-in-our-Model" class="headerlink" title="3.2.3 Applications of Attention in our Model"></a>3.2.3 Applications of Attention in our Model</h4><ul><li><p>In “encoder-decoder attention” layers, the queries come from the previous decoder layer,and the memory keys and values come from the output of the encoder. This allows every position in the decoder to attend over all positions in the input sequence. 在Decoder中，$Q$矩阵来自Decoder的前一层，$K,V$来自Encoder的输出。</p><p><img src="image-20211027214240983.png" alt="Overall Architecture"></p></li><li><p>The encoder contains self-attention layers. In a self-attention layer all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder. Each position in the encoder can attend to all positions in the previous layer of the encoder. 在Encoder中，$Q,K,V$矩阵均来自上一层Encoder的输出。</p></li><li><p>We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this<br>inside of scaled dot-product attention by masking out (setting to -∞ ) all values in the input of the $softmax$ which correspond to illegal connections. Decoder应该避免看到未来的信息，只考虑当前的信息，所以对所有未来的信息，通过设置$-\infty$将其过滤掉。有关自回归性，<a href="https://blog.csdn.net/qq_27590277/article/details/107274212?ops_request_misc=&request_id=&biz_id=102&utm_term=seq%E8%87%AA%E5%9B%9E%E5%BD%92%E6%80%A7&utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduweb~default-2-107274212.pc_search_result_hbase_insert&spm=1018.2226.3001.4187">可以参考这篇文章。</a></p></li></ul><h3 id="3-3-Position-wise-Feed-Forward-Networks"><a href="#3-3-Position-wise-Feed-Forward-Networks" class="headerlink" title="3.3 Position-wise Feed-Forward Networks"></a>3.3 Position-wise Feed-Forward Networks</h3><ul><li>In addition to attention sub-layers, each of the layers in our encoder and  decoder contains a fully connected feed-forward network, which is applied to  each position separately and identically. This consists of two linear  transformations with a ReLU activation in between. 每个Encoder和Decoder都包含一个全连接前向反馈网络，包括两个线性变换，中间用ReLU激活。同一层每个位置的信息对应的FFN一样，不同层的FFN不一样。</li><li>$FFN(x)=ReLU(xW_1+b_1)W_2+b_2$. 层与层之间使用不同的参数。</li></ul><h3 id="3-4-Embedding-and-Softmax"><a href="#3-4-Embedding-and-Softmax" class="headerlink" title="3.4 Embedding and Softmax"></a>3.4 Embedding and Softmax</h3><ul><li>Similarly to other sequence transduction models, we use learned embeddings to convert the input tokens and output tokens to vectors of dimension $d_{model}$. 和其他序列模型类似，Transformer使用学习到的嵌入层将输入和输出信息转换为向量。</li><li>We also use the usual learned linear transformation and $softmax$ function to  convert the decoder output to predicted next-token probabilities. FFN需要经过训练，通过$Softmax$输出概率。</li><li>In our model, we share the same weight matrix between the two embedding layers and the pre-softmax linear transformation.  在将源语言和目标语言转换为向量的过程中，原文让他们共享一个词嵌入矩阵。对于欧洲语系，这是可行的，因为它们之间有许多相似之处。对于中英文翻译，则没有必要共享参数。</li><li>In the embedding layers, we multiply those weights by $\sqrt{d_{model}}$.  词嵌入矩阵 * $\sqrt{d_{model}}$.</li></ul><h3 id="3-5-Positional-Encoding-PE"><a href="#3-5-Positional-Encoding-PE" class="headerlink" title="3.5 Positional Encoding(PE)"></a>3.5 Positional Encoding(PE)</h3><ul><li>Since our model contains no recurrence and no convolution, in order for the  model to make use of the order of the sequence, we must inject some information  about the relative or absolute position of the tokens in the sequence. Transformer没有考虑到不同位置下的信息差异，即相同的信息不同的排列，得到的结果是一样的。为了能够考虑位置因素，需要加入位置信息，即Positional Encoding.</li><li> The positional encodings have the same dimension $d_{model}$ as the embeddings, so that the two can be summed. Positional encodings和embeddings的维度一致，故可以直接相加。这样，输入向量就具备了位置信息。</li><li>$PE_{(pos,2i)}=sin(pos/10000^{2i/d_{model}}),PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})$. $pos$指位置，$i$指维度。</li><li>We chose this function because we hypothesized it would allow the model to easily learn to attend by<br>relative positions. 以上公式可以认为是经验公式，不是数学推导的结果。</li></ul><h2 id="4-Code代码"><a href="#4-Code代码" class="headerlink" title="4 Code代码"></a>4 Code代码</h2><ul><li>参考网址:<a href="http://nlp.seas.harvard.edu/2018/04/03/attention.html">The Annotated Transformer (harvard.edu)</a>,<a href="https://zhuanlan.zhihu.com/p/107889011">The Annotated Transformer的中文注释版（1） - 知乎 (zhihu.com)</a></li></ul><img src="https://pic4.zhimg.com/80/v2-a50a353fb2277abc01e0ef14607a499f_720w.jpg" alt="Model Architecture" style="zoom:80%;" /><h3 id="4-0-packages"><a href="#4-0-packages" class="headerlink" title="4.0 packages"></a>4.0 packages</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># !pip install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl </span><span class="token comment"># !pip install numpy matplotlib spacy torchtext seaborn</span><span class="token keyword">import</span> numpy <span class="token keyword">as</span> np<span class="token keyword">import</span> torch<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn <span class="token keyword">as</span> nn<span class="token keyword">import</span> torch<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>functional <span class="token keyword">as</span> F<span class="token keyword">import</span> math<span class="token punctuation">,</span>copy<span class="token punctuation">,</span>time<span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable<span class="token keyword">import</span> matplotlib<span class="token punctuation">.</span>pyplot <span class="token keyword">as</span> plt<span class="token keyword">import</span> seabornseaborn<span class="token punctuation">.</span>set_context<span class="token punctuation">(</span>context<span class="token operator">=</span><span class="token string">"talk"</span><span class="token punctuation">)</span> <span class="token comment"># seaborn只在最后可视化self-attention的时候用到，</span><span class="token comment"># 可以先不管或者注释掉这两行</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-1-Embeddings"><a href="#4-1-Embeddings" class="headerlink" title="4.1 Embeddings"></a>4.1 Embeddings</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Embeddings</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>d_model<span class="token punctuation">,</span>vocab<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment">#d_model=512, vocab=当前语言的词表大小</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>Embeddings<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>    self<span class="token punctuation">.</span>lut<span class="token operator">=</span>nn<span class="token punctuation">.</span>Embedding<span class="token punctuation">(</span>vocab<span class="token punctuation">,</span>d_model<span class="token punctuation">)</span>     <span class="token comment"># one-hot转词嵌入，这里有一个待训练的矩阵E，大小是vocab*d_model</span>    self<span class="token punctuation">.</span>d_model<span class="token operator">=</span>d_model <span class="token comment"># 512</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment"># x ~ (batch.size, sequence.length, one-hot), </span>     <span class="token comment">#one-hot大小=vocab，当前语言的词表大小</span>     <span class="token keyword">return</span> self<span class="token punctuation">.</span>lut<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token operator">*</span>math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>self<span class="token punctuation">.</span>d_model<span class="token punctuation">)</span>      <span class="token comment"># 得到的10*512词嵌入矩阵，主动乘以sqrt(512)=22.6，</span>     <span class="token comment">#这里的输出的tensor大小类似于(batch.size, sequence.length, 512)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>默认情况下，源语言和目标语言的Embedding不共享参数。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">if</span> <span class="token boolean">False</span><span class="token punctuation">:</span>    model<span class="token punctuation">.</span>src_embed<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>lut<span class="token punctuation">.</span>weight<span class="token operator">=</span>model<span class="token punctuation">.</span>tgt_embeddings<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>lut<span class="token punctuation">.</span>weight    model<span class="token punctuation">.</span>generator<span class="token punctuation">.</span>lut<span class="token punctuation">.</span>weight<span class="token operator">=</span>model<span class="token punctuation">.</span>tgt_embed<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">.</span>lut<span class="token punctuation">.</span>weight<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><h3 id="4-2-Positional-Encoding-PE"><a href="#4-2-Positional-Encoding-PE" class="headerlink" title="4.2 Positional Encoding(PE)"></a>4.2 Positional Encoding(PE)</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PositionalEncoding</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token triple-quoted-string string">"""  Implement the PE function.  """</span>  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token punctuation">,</span> max_len<span class="token operator">=</span><span class="token number">5000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment">#d_model=512,dropout=0.1,</span>    <span class="token comment">#max_len=5000代表事先准备好长度为5000的序列的位置编码，其实没必要，</span>    <span class="token comment">#一般100或者200足够了。</span>    <span class="token builtin">super</span><span class="token punctuation">(</span>PositionalEncoding<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>     self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>     <span class="token comment"># Compute the positional encodings once in log space. </span>    pe <span class="token operator">=</span> torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>max_len<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>     <span class="token comment">#(5000,512)矩阵，保持每个位置的位置编码，一共5000个位置，</span>    <span class="token comment">#每个位置用一个512维度向量来表示其位置编码</span>    position <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> max_len<span class="token punctuation">)</span><span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span>     <span class="token comment"># (5000) -> (5000,1)</span>    <span class="token comment"># arrange(start, end, step)</span>    div_term <span class="token operator">=</span> torch<span class="token punctuation">.</span>exp<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token operator">*</span>       <span class="token operator">-</span><span class="token punctuation">(</span>math<span class="token punctuation">.</span>log<span class="token punctuation">(</span><span class="token number">10000.0</span><span class="token punctuation">)</span> <span class="token operator">/</span> d_model<span class="token punctuation">)</span><span class="token punctuation">)</span>       <span class="token comment"># (0,2,…, 4998)一共准备2500个值，供sin, cos调用</span>    pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>sin<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span> <span class="token comment"># 偶数下标的位置</span>    pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">:</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">=</span> torch<span class="token punctuation">.</span>cos<span class="token punctuation">(</span>position <span class="token operator">*</span> div_term<span class="token punctuation">)</span> <span class="token comment"># 奇数下标的位置</span>    pe <span class="token operator">=</span> pe<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>     <span class="token comment"># (5000, 512) -> (1, 5000, 512) 为batch.size留出位置</span>    self<span class="token punctuation">.</span>register_buffer<span class="token punctuation">(</span><span class="token string">'pe'</span><span class="token punctuation">,</span> pe<span class="token punctuation">)</span>   <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>     x <span class="token operator">=</span> x <span class="token operator">+</span> Variable<span class="token punctuation">(</span>self<span class="token punctuation">.</span>pe<span class="token punctuation">[</span><span class="token punctuation">:</span><span class="token punctuation">,</span> <span class="token punctuation">:</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>     <span class="token comment"># 接受1.Embeddings的词嵌入结果x，</span>    <span class="token comment">#然后把自己的位置编码pe，封装成torch的Variable(不需要梯度)，加上去。</span>    <span class="token comment">#例如，假设x是(30,10,512)的一个tensor，</span>    <span class="token comment">#30是batch.size, 10是该batch的序列长度, 512是每个词的词嵌入向量；</span>    <span class="token comment">#则该行代码的第二项是(1, min(10, 5000), 512)=(1,10,512)，</span>    <span class="token comment">#在具体相加的时候，会扩展(1,10,512)为(30,10,512)，</span>    <span class="token comment">#保证一个batch中的30个序列，都使用（叠加）一样的位置编码。</span>    <span class="token keyword">return</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment"># 增加一次dropout操作</span><span class="token comment"># 注意，位置编码不会更新，是写死的，所以这个class里面没有可训练的参数。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>4.1和4.2按如下方式连接起来</p><pre class="line-numbers language-python" data-language="python"><code class="language-python">nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Embeddings<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span>src_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span>PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span>dropout<span class="token punctuation">)</span><span class="token punctuation">)</span> <span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><h3 id="4-3-Multi-Head-Attention"><a href="#4-3-Multi-Head-Attention" class="headerlink" title="4.3 Multi-Head Attention"></a>4.3 Multi-Head Attention</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">attention</span><span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span> <span class="token comment"># query, key, value的形状类似于(30, 8, 10, 64), (30, 8, 11, 64), </span><span class="token comment">#(30, 8, 11, 64)，例如30是batch.size，即当前batch中有多少一个序列；</span><span class="token comment"># 8=head.num，注意力头的个数；</span><span class="token comment"># 10=目标序列中词的个数，64是每个词对应的向量表示；</span><span class="token comment"># 11=源语言序列传过来的memory中，当前序列的词的个数，</span><span class="token comment"># 64是每个词对应的向量表示。</span><span class="token comment"># 类似于，这里假定query来自target language sequence；</span><span class="token comment"># key和value都来自source language sequence.</span>  <span class="token string">"Compute 'Scaled Dot Product Attention'"</span>   d_k <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># 64=d_k</span>  scores <span class="token operator">=</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">2</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span> <span class="token operator">/</span>     math<span class="token punctuation">.</span>sqrt<span class="token punctuation">(</span>d_k<span class="token punctuation">)</span> <span class="token comment"># 先是(30,8,10,64)和(30, 8, 64, 11)相乘，</span>    <span class="token comment">#（注意是最后两个维度相乘）得到(30,8,10,11)，</span>    <span class="token comment">#代表10个目标语言序列中每个词和11个源语言序列的分别的“亲密度”。</span>    <span class="token comment">#然后除以sqrt(d_k)=8，防止过大的亲密度。</span>    <span class="token comment">#这里的scores的shape是(30, 8, 10, 11)</span>  <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>     scores <span class="token operator">=</span> scores<span class="token punctuation">.</span>masked_fill<span class="token punctuation">(</span>mask <span class="token operator">==</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1e9</span><span class="token punctuation">)</span>     <span class="token comment">#使用mask，对已经计算好的scores，按照mask矩阵，填-1e9，</span>    <span class="token comment">#然后在下一步计算softmax的时候，被设置成-1e9的数对应的值~0,被忽视</span>  p_attn <span class="token operator">=</span> F<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>scores<span class="token punctuation">,</span> dim <span class="token operator">=</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>     <span class="token comment">#对scores的最后一个维度执行softmax，得到的还是一个tensor, </span>    <span class="token comment">#(30, 8, 10, 11)</span>  <span class="token keyword">if</span> dropout <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>     p_attn <span class="token operator">=</span> dropout<span class="token punctuation">(</span>p_attn<span class="token punctuation">)</span> <span class="token comment">#执行一次dropout</span>  <span class="token keyword">return</span> torch<span class="token punctuation">.</span>matmul<span class="token punctuation">(</span>p_attn<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">,</span> p_attn<span class="token comment">#返回的第一项，是(30,8,10, 11)乘以（最后两个维度相乘）</span><span class="token comment">#value=(30,8,11,64)，得到的tensor是(30,8,10,64)，</span><span class="token comment">#和query的最初的形状一样。另外，返回p_attn，形状为(30,8,10,11). </span><span class="token comment">#注意，这里返回p_attn主要是用来可视化显示多头注意力机制。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">MultiHeadedAttention</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>   <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> h<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment"># h=8, d_model=512</span>    <span class="token string">"Take in model size and number of heads."</span>     <span class="token builtin">super</span><span class="token punctuation">(</span>MultiHeadedAttention<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token keyword">assert</span> d_model <span class="token operator">%</span> h <span class="token operator">==</span> <span class="token number">0</span> <span class="token comment"># We assume d_v always equals d_k 512%8=0</span>    self<span class="token punctuation">.</span>d_k <span class="token operator">=</span> d_model <span class="token operator">//</span> h <span class="token comment"># d_k=512//8=64</span>    self<span class="token punctuation">.</span>h <span class="token operator">=</span> h <span class="token comment">#8</span>    self<span class="token punctuation">.</span>linears <span class="token operator">=</span> clones<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">4</span><span class="token punctuation">)</span>     <span class="token comment">#定义四个Linear networks, 每个的大小是(512, 512)的，</span>    <span class="token comment">#每个Linear network里面有两类可训练参数，Weights，其大小为512*512</span>    <span class="token comment">#以及biases，其大小为512=d_model。</span>    self<span class="token punctuation">.</span>attn <span class="token operator">=</span> <span class="token boolean">None</span>     self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>p<span class="token operator">=</span>dropout<span class="token punctuation">)</span>  <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token comment"># 注意，输入query的形状类似于(30, 10, 512)，</span>   <span class="token comment"># key.size() ~ (30, 11, 512), </span>   <span class="token comment">#以及value.size() ~ (30, 11, 512)</span>        <span class="token keyword">if</span> mask <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span> <span class="token comment"># Same mask applied to all h heads. </span>      mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">)</span> <span class="token comment"># mask下回细细分解。</span>    nbatches <span class="token operator">=</span> query<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span> <span class="token comment">#e.g., nbatches=30</span>    <span class="token comment"># 1) Do all the linear projections in batch from </span>    <span class="token comment">#d_model => h x d_k </span>    query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value <span class="token operator">=</span> <span class="token punctuation">[</span>l<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span>nbatches<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h<span class="token punctuation">,</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span>      <span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span> <span class="token keyword">for</span> l<span class="token punctuation">,</span> x <span class="token keyword">in</span>       <span class="token builtin">zip</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>linears<span class="token punctuation">,</span> <span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">]</span>       <span class="token comment"># 这里是前三个Linear Networks的具体应用，</span>      <span class="token comment">#例如query=(30,10, 512) -> Linear network -> (30, 10, 512) </span>      <span class="token comment">#-> view -> (30,10, 8, 64) -> transpose(1,2) -> (30, 8, 10, 64)</span>      <span class="token comment">#，其他的key和value也是类似地，</span>      <span class="token comment">#从(30, 11, 512) -> (30, 8, 11, 64)。</span>    <span class="token comment"># 2) Apply attention on all the projected vectors in batch. </span>    x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>attn <span class="token operator">=</span> attention<span class="token punctuation">(</span>query<span class="token punctuation">,</span> key<span class="token punctuation">,</span> value<span class="token punctuation">,</span> mask<span class="token operator">=</span>mask<span class="token punctuation">,</span>       dropout<span class="token operator">=</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">)</span>       <span class="token comment">#调用上面定义好的attention函数，输出的x形状为(30, 8, 10, 64)；</span>      <span class="token comment">#attn的形状为(30, 8, 10=target.seq.len, 11=src.seq.len)</span>    <span class="token comment"># 3) "Concat" using a view and apply a final linear. </span>    x <span class="token operator">=</span> x<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span>contiguous<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>      view<span class="token punctuation">(</span>nbatches<span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>h <span class="token operator">*</span> self<span class="token punctuation">.</span>d_k<span class="token punctuation">)</span>       <span class="token comment"># x ~ (30, 8, 10, 64) -> transpose(1,2) -> </span>      <span class="token comment">#(30, 10, 8, 64) -> contiguous() and view -> </span>      <span class="token comment">#(30, 10, 8*64) = (30, 10, 512)</span><span class="token keyword">return</span> self<span class="token punctuation">.</span>linears<span class="token punctuation">[</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span> <span class="token comment">#执行第四个Linear network，把(30, 10, 512)经过一次linear network，</span><span class="token comment">#得到(30, 10, 512).</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-7-LayerNorm"><a href="#4-7-LayerNorm" class="headerlink" title="4.7 LayerNorm"></a>4.7 LayerNorm</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">LayerNorm</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Construct a layernorm module (See citation for details)."</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> features<span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e-6</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># features=d_model=512, eps=epsilon 用于分母的非0化平滑</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>LayerNorm<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>a_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>ones<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># a_2 是一个可训练参数向量，(512)</span>        self<span class="token punctuation">.</span>b_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Parameter<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>zeros<span class="token punctuation">(</span>features<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># b_2 也是一个可训练参数向量, (512)</span>        self<span class="token punctuation">.</span>eps <span class="token operator">=</span> eps    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># x 的形状为(batch.size, sequence.len, 512)</span>        mean <span class="token operator">=</span> x<span class="token punctuation">.</span>mean<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>         <span class="token comment"># 对x的最后一个维度，取平均值，得到tensor (batch.size, seq.len, 1)</span>        std <span class="token operator">=</span> x<span class="token punctuation">.</span>std<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        <span class="token comment"># 对x的最后一个维度，取标准方差，得(batch.size, seq.len, 1)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>a_2 <span class="token operator">*</span> <span class="token punctuation">(</span>x <span class="token operator">-</span> mean<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token punctuation">(</span>std <span class="token operator">+</span> self<span class="token punctuation">.</span>eps<span class="token punctuation">)</span> <span class="token operator">+</span> self<span class="token punctuation">.</span>b_2        <span class="token comment"># 本质上类似于（x-mean)/std，不过这里加入了两个可训练向量</span>        <span class="token comment"># a_2 and b_2，以及分母上增加一个极小值epsilon，用来防止std为0</span>        <span class="token comment"># 的时候的除法溢出</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-4-SubLayer-Connection"><a href="#4-4-SubLayer-Connection" class="headerlink" title="4.4 SubLayer Connection"></a>4.4 SubLayer Connection</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SublayerConnection</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    A residual connection followed by a layer norm.    Note for code simplicity the norm is first as opposed to last.    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># size=d_model=512; dropout=0.1</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>size<span class="token punctuation">)</span> <span class="token comment"># (512)，用来定义a_2和b_2</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> sublayer<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token string">"Apply residual connection to any sublayer with the "</span>        <span class="token string">"same size."</span>        <span class="token comment"># x is alike (batch.size, sequence.len, 512)</span>        <span class="token comment"># sublayer是一个具体的MultiHeadAttention</span>        <span class="token comment">#或者PositionwiseFeedForward对象</span>        <span class="token keyword">return</span> x <span class="token operator">+</span> self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>sublayer<span class="token punctuation">(</span>self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># return x + self.dropout(self.norm(sublayer(x)))</span>        <span class="token comment"># x (30, 10, 512) -> norm (LayerNorm) -> (30, 10, 512)</span>        <span class="token comment"># -> sublayer (MultiHeadAttention or PositionwiseFeedForward)</span>        <span class="token comment"># -> (30, 10, 512) -> dropout -> (30, 10, 512)</span>                <span class="token comment"># 然后输入的x（没有走sublayer) + 上面的结果，</span>        <span class="token comment">#即实现了残差相加的功能</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-5-Position-wise-Feed-Forward-Network"><a href="#4-5-Position-wise-Feed-Forward-Network" class="headerlink" title="4.5 Position-wise Feed Forward Network"></a>4.5 Position-wise Feed Forward Network</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">PositionwiseFeedForward</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Implements FFN equation."</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># d_model = 512</span>        <span class="token comment"># d_ff = 2048 = 512*4</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>PositionwiseFeedForward<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>w_1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">)</span>        <span class="token comment"># 构建第一个全连接层，(512, 2048)，其中有两种可训练参数：</span>        <span class="token comment"># weights矩阵，(512, 2048)，以及</span>        <span class="token comment"># biases偏移向量, (2048)</span>        self<span class="token punctuation">.</span>w_2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_ff<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span>        <span class="token comment"># 构建第二个全连接层, (2048, 512)，两种可训练参数：</span>        <span class="token comment"># weights矩阵，(2048, 512)，以及</span>        <span class="token comment"># biases偏移向量, (512)</span>        self<span class="token punctuation">.</span>dropout <span class="token operator">=</span> nn<span class="token punctuation">.</span>Dropout<span class="token punctuation">(</span>dropout<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># x shape = (batch.size, sequence.len, 512)</span>        <span class="token comment"># 例如, (30, 10, 512)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>w_2<span class="token punctuation">(</span>self<span class="token punctuation">.</span>dropout<span class="token punctuation">(</span>F<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>self<span class="token punctuation">.</span>w_1<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># x (30, 10, 512) -> self.w_1 -> (30, 10, 2048)</span>        <span class="token comment"># -> relu -> (30, 10, 2048) </span>        <span class="token comment"># -> dropout -> (30, 10, 2048)</span>        <span class="token comment"># -> self.w_2 -> (30, 10, 512)是输出的shape</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token triple-quoted-string string">"""下面这个clones方法，实现一个网络的深copy，也就是说一个新的对象，和原来的对象，完全分离，不分享任何存储空间：（从而保证可训练参数，都有自己的取值，梯度）"""</span><span class="token keyword">def</span> <span class="token function">clones</span><span class="token punctuation">(</span>module<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Produce N identical layers."</span>    <span class="token keyword">return</span> nn<span class="token punctuation">.</span>ModuleList<span class="token punctuation">(</span><span class="token punctuation">[</span>copy<span class="token punctuation">.</span>deepcopy<span class="token punctuation">(</span>module<span class="token punctuation">)</span> <span class="token keyword">for</span> _ <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>N<span class="token punctuation">)</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-6-EncoderLayer"><a href="#4-6-EncoderLayer" class="headerlink" title="4.6 EncoderLayer"></a>4.6 EncoderLayer</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Encoder is made up of self-attn and "</span>    <span class="token string">"feed forward (defined below)"</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> self_attn<span class="token punctuation">,</span> feed_forward<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># size=d_model=512</span>        <span class="token comment"># self_attn = MultiHeadAttention对象, first sublayer</span>        <span class="token comment"># feed_forward = PositionwiseFeedForward对象，second sublayer</span>        <span class="token comment"># dropout = 0.1 (e.g.)</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> self_attn        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> feed_forward        self<span class="token punctuation">.</span>sublayer <span class="token operator">=</span> clones<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">(</span>size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 使用深度克隆方法，完整地复制出来两个SublayerConnection</span>        self<span class="token punctuation">.</span>size <span class="token operator">=</span> size <span class="token comment"># 512</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token string">"Follow Figure 1 (left) for connections."</span>        <span class="token comment"># x shape = (30, 10, 512)</span>        <span class="token comment"># mask 是(batch.size, 10,10)的矩阵，类似于当前一个词w，有哪些词是w可见的</span>        <span class="token comment"># 源语言的序列的话，所有其他词都可见，除了"&lt;blank>"这样的填充；</span>        <span class="token comment"># 目标语言的序列的话，所有w的左边的词，都可见。</span>        <span class="token comment"># 匿名函数</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># x (30, 10, 512) -> self_attn (MultiHeadAttention) </span>        <span class="token comment"># shape is same (30, 10, 512) -> SublayerConnection </span>        <span class="token comment"># -> (30, 10, 512)</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span>        <span class="token comment"># x 和feed_forward对象一起，给第二个SublayerConnection</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-8-Encoder"><a href="#4-8-Encoder" class="headerlink" title="4.8 Encoder"></a>4.8 Encoder</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Encoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Core encoder is a stack of N layers"</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># layer = one EncoderLayer object, N=6</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Encoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> clones<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span>         <span class="token comment"># 深copy，N=6，</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>size<span class="token punctuation">)</span>        <span class="token comment"># 定义一个LayerNorm，layer.size=d_model=512</span>        <span class="token comment"># 其中有两个可训练参数a_2和b_2</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token string">"Pass the input (and mask) through each layer in turn."</span>        <span class="token comment"># x is alike (30, 10, 512)</span>        <span class="token comment"># (batch.size, sequence.len, d_model)</span>        <span class="token comment"># mask是类似于(batch.size, 10, 10)的矩阵</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> mask<span class="token punctuation">)</span>            <span class="token comment"># 进行六次EncoderLayer操作</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># 最后做一次LayerNorm，最后的输出也是(30, 10, 512) shape</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-9-DecoderLayer"><a href="#4-9-DecoderLayer" class="headerlink" title="4.9 DecoderLayer"></a>4.9 DecoderLayer</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">DecoderLayer</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Decoder is made of self-attn, src-attn, "</span>    <span class="token string">"and feed forward (defined below)"</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> size<span class="token punctuation">,</span> self_attn<span class="token punctuation">,</span> src_attn<span class="token punctuation">,</span>       feed_forward<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment"># size = d_model=512,</span>      <span class="token comment"># self_attn = one MultiHeadAttention object，目标语言序列的</span>      <span class="token comment"># src_attn = second MultiHeadAttention object, 目标语言序列</span>      <span class="token comment"># 和源语言序列之间的</span>      <span class="token comment"># feed_forward 一个全连接层</span>      <span class="token comment"># dropout = 0.1</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>size <span class="token operator">=</span> size <span class="token comment"># 512</span>        self<span class="token punctuation">.</span>self_attn <span class="token operator">=</span> self_attn        self<span class="token punctuation">.</span>src_attn <span class="token operator">=</span> src_attn        self<span class="token punctuation">.</span>feed_forward <span class="token operator">=</span> feed_forward        self<span class="token punctuation">.</span>sublayer <span class="token operator">=</span> clones<span class="token punctuation">(</span>SublayerConnection<span class="token punctuation">(</span>size<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">)</span>        <span class="token comment"># 需要三个SublayerConnection, 分别在</span>        <span class="token comment"># self.self_attn, self.src_attn, 和self.feed_forward</span>        <span class="token comment"># 的后边</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token string">"Follow Figure 1 (right) for connections."</span>        m <span class="token operator">=</span> memory <span class="token comment"># (batch.size, sequence.len, 512) </span>        <span class="token comment"># 来自源语言序列的Encoder之后的输出，作为memory</span>        <span class="token comment"># 供目标语言的序列检索匹配：（类似于alignment in SMT)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>           <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>self_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> x<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 通过一个匿名函数，来实现目标序列的自注意力编码</span>        <span class="token comment"># 结果扔给sublayer[0]:SublayerConnection</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span>           <span class="token keyword">lambda</span> x<span class="token punctuation">:</span> self<span class="token punctuation">.</span>src_attn<span class="token punctuation">(</span>x<span class="token punctuation">,</span> m<span class="token punctuation">,</span> m<span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 通过第二个匿名函数，来实现目标序列和源序列的注意力计算</span>        <span class="token comment"># 结果扔给sublayer[1]:SublayerConnection</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>sublayer<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>feed_forward<span class="token punctuation">)</span>        <span class="token comment"># 走一个全连接层，然后</span>        <span class="token comment"># 结果扔给sublayer[2]:SublayerConnection</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-10-Decoder"><a href="#4-10-Decoder" class="headerlink" title="4.10 Decoder"></a>4.10 Decoder</h3><p><img src="https://pic1.zhimg.com/80/v2-b1bf4ef59f01b73c55f5f66fcc99d134_720w.jpg" alt="img"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Decoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Generic N layer decoder with masking."</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># layer = DecoderLayer object</span>        <span class="token comment"># N = 6</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Decoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>layers <span class="token operator">=</span> clones<span class="token punctuation">(</span>layer<span class="token punctuation">,</span> N<span class="token punctuation">)</span>        <span class="token comment"># 深度copy六次DecoderLayer</span>        self<span class="token punctuation">.</span>norm <span class="token operator">=</span> LayerNorm<span class="token punctuation">(</span>layer<span class="token punctuation">.</span>size<span class="token punctuation">)</span>        <span class="token comment"># 初始化一个LayerNorm</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">for</span> layer <span class="token keyword">in</span> self<span class="token punctuation">.</span>layers<span class="token punctuation">:</span>            x <span class="token operator">=</span> layer<span class="token punctuation">(</span>x<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span>            <span class="token comment"># 执行六次DecoderLayer</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>norm<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># 执行一次LayerNorm</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-11-Generator"><a href="#4-11-Generator" class="headerlink" title="4.11 Generator"></a>4.11 Generator</h3><p><img src="https://pic2.zhimg.com/80/v2-55395e797e8c887c02836f0211633169_720w.jpg" alt="img"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">Generator</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Define standard linear + softmax generation step."</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> d_model<span class="token punctuation">,</span> vocab<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># d_model=512</span>        <span class="token comment"># vocab = 目标语言词表大小</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Generator<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>proj <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> vocab<span class="token punctuation">)</span>        <span class="token comment"># 定义一个全连接层，可训练参数个数是(512 * trg_vocab_size) + </span>        <span class="token comment"># trg_vocab_size</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> F<span class="token punctuation">.</span>log_softmax<span class="token punctuation">(</span>self<span class="token punctuation">.</span>proj<span class="token punctuation">(</span>x<span class="token punctuation">)</span><span class="token punctuation">,</span> dim<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># x 类似于 (batch.size, sequence.length, 512)</span>        <span class="token comment"># -> proj 全连接层 (30, 10, trg_vocab_size) = logits</span>        <span class="token comment"># 对最后一个维度执行log_soft_max</span>        <span class="token comment"># 得到(30, 10, trg_vocab_size)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-12-EncoderDecoder"><a href="#4-12-EncoderDecoder" class="headerlink" title="4.12 EncoderDecoder"></a>4.12 EncoderDecoder</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">EncoderDecoder</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""    A standard Encoder-Decoder architecture.     Base for this and many other models.    """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> encoder<span class="token punctuation">,</span> decoder<span class="token punctuation">,</span>       src_embed<span class="token punctuation">,</span> tgt_embed<span class="token punctuation">,</span> generator<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>EncoderDecoder<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>encoder <span class="token operator">=</span> encoder        <span class="token comment"># Encoder对象</span>        self<span class="token punctuation">.</span>decoder <span class="token operator">=</span> decoder        <span class="token comment"># Decoder对象</span>        self<span class="token punctuation">.</span>src_embed <span class="token operator">=</span> src_embed        <span class="token comment"># 源语言序列的编码，包括词嵌入和位置编码</span>        self<span class="token punctuation">.</span>tgt_embed <span class="token operator">=</span> tgt_embed        <span class="token comment"># 目标语言序列的编码，包括词嵌入和位置编码</span>        self<span class="token punctuation">.</span>generator <span class="token operator">=</span> generator        <span class="token comment"># 生成器</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> tgt<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token string">"Take in and process masked src and target sequences."</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decode<span class="token punctuation">(</span>self<span class="token punctuation">.</span>encode<span class="token punctuation">(</span>src<span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span><span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span>                            tgt<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span>        <span class="token comment"># 先对源语言序列进行编码，</span>        <span class="token comment"># 结果作为memory传递给目标语言的编码器</span>    <span class="token keyword">def</span> <span class="token function">encode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> src<span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># src = (batch.size, seq.length)</span>        <span class="token comment"># src_mask 负责对src加掩码</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>encoder<span class="token punctuation">(</span>self<span class="token punctuation">.</span>src_embed<span class="token punctuation">(</span>src<span class="token punctuation">)</span><span class="token punctuation">,</span> src_mask<span class="token punctuation">)</span>        <span class="token comment"># 对源语言序列进行编码，得到的结果为</span>        <span class="token comment"># (batch.size, seq.length, 512)的tensor</span>    <span class="token keyword">def</span> <span class="token function">decode</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">return</span> self<span class="token punctuation">.</span>decoder<span class="token punctuation">(</span>self<span class="token punctuation">.</span>tgt_embed<span class="token punctuation">(</span>tgt<span class="token punctuation">)</span><span class="token punctuation">,</span>           memory<span class="token punctuation">,</span> src_mask<span class="token punctuation">,</span> tgt_mask<span class="token punctuation">)</span>        <span class="token comment"># 对目标语言序列进行编码，得到的结果为</span>        <span class="token comment"># (batch.size, seq.length, 512)的tensor</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="4-13-model"><a href="#4-13-model" class="headerlink" title="4.13 model"></a>4.13 model</h3><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">def</span> <span class="token function">make_model</span><span class="token punctuation">(</span>src_vocab<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">,</span> N<span class="token operator">=</span><span class="token number">6</span><span class="token punctuation">,</span>                d_model<span class="token operator">=</span><span class="token number">512</span><span class="token punctuation">,</span> d_ff<span class="token operator">=</span><span class="token number">2048</span><span class="token punctuation">,</span> h<span class="token operator">=</span><span class="token number">8</span><span class="token punctuation">,</span> dropout<span class="token operator">=</span><span class="token number">0.1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token string">"Helper: Construct a model from hyperparameters."</span>    <span class="token comment"># src_vocab = 源语言词表大小</span>    <span class="token comment"># tgt_vocab = 目标语言词表大小</span>        c <span class="token operator">=</span> copy<span class="token punctuation">.</span>deepcopy <span class="token comment"># 对象的深度copy/clone</span>    attn <span class="token operator">=</span> MultiHeadedAttention<span class="token punctuation">(</span>h<span class="token punctuation">,</span> d_model<span class="token punctuation">)</span> <span class="token comment"># 8, 512</span>    <span class="token comment"># 构造一个MultiHeadAttention对象</span>        ff <span class="token operator">=</span> PositionwiseFeedForward<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> d_ff<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>    <span class="token comment"># 512, 2048, 0.1</span>    <span class="token comment"># 构造一个feed forward对象</span>    position <span class="token operator">=</span> PositionalEncoding<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> dropout<span class="token punctuation">)</span>    <span class="token comment"># 位置编码</span>    model <span class="token operator">=</span> EncoderDecoder<span class="token punctuation">(</span>        Encoder<span class="token punctuation">(</span>EncoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>ff<span class="token punctuation">)</span><span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">,</span>        Decoder<span class="token punctuation">(</span>DecoderLayer<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>attn<span class="token punctuation">)</span><span class="token punctuation">,</span>                              c<span class="token punctuation">(</span>ff<span class="token punctuation">)</span><span class="token punctuation">,</span> dropout<span class="token punctuation">)</span><span class="token punctuation">,</span> N<span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Embeddings<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> src_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>position<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span>Embeddings<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">)</span><span class="token punctuation">,</span> c<span class="token punctuation">(</span>position<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">,</span>        Generator<span class="token punctuation">(</span>d_model<span class="token punctuation">,</span> tgt_vocab<span class="token punctuation">)</span><span class="token punctuation">)</span>    <span class="token comment"># This was important from their code. </span>    <span class="token comment"># Initialize parameters with Glorot / fan_avg.</span>    <span class="token keyword">for</span> p <span class="token keyword">in</span> model<span class="token punctuation">.</span>parameters<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token keyword">if</span> p<span class="token punctuation">.</span>dim<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token operator">></span> <span class="token number">1</span><span class="token punctuation">:</span>            nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span>xavier_uniform<span class="token punctuation">(</span>p<span class="token punctuation">)</span>    <span class="token keyword">return</span> model <span class="token comment"># EncoderDecoder 对象</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>On the Detection of Digital Face Manipulation(2020 CVPR)</title>
      <link href="/2021/06/03/on-the-detection-of-digital-face-manipulation-2020-cvpr/"/>
      <url>/2021/06/03/on-the-detection-of-digital-face-manipulation-2020-cvpr/</url>
      
        <content type="html"><![CDATA[<h1 id="论文解读：On-the-Detection-of-Digital-Face-Manipulation（2020-CVPR"><a href="#论文解读：On-the-Detection-of-Digital-Face-Manipulation（2020-CVPR" class="headerlink" title="论文解读：On the Detection of Digital Face Manipulation（2020 CVPR)"></a>论文解读：On the Detection of Digital Face Manipulation（2020 CVPR)</h1><h2 id="一-创新点"><a href="#一-创新点" class="headerlink" title="一.创新点"></a>一.创新点</h2><ol><li>制作了一个综合的fakeface数据集，包括0.8M真实人脸和1.8M由不同方法生成的伪人脸。</li><li>提出一种新颖的基于注意力的层，用于提高分类性能并产生指示被操纵的面部区域的注意力图。</li><li>提出一种新的度量，称为IINC(Inverse Intersection Non-containment )，用于评估注意力图，产生比现有度量更一致的评估。</li></ol><h2 id="二-注意力机制"><a href="#二-注意力机制" class="headerlink" title="二.注意力机制"></a>二.注意力机制</h2><h3 id="1-特点"><a href="#1-特点" class="headerlink" title="1.特点"></a>1.特点</h3><ol><li>并行计算</li><li>考虑了序列的前后关系</li><li>参数共享</li></ol><p>更多参考：<a href="https://blog.csdn.net/xys430381_1/article/details/89323444?ops_request_misc=%7B%22request_id%22:%22162237886216780269816338%22,%22scm%22:%2220140713.130102334.pc_all.%22%7D&request_id=162237886216780269816338&biz_id=0&utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_v2~rank_v29-1-89323444.pc_search_result_cache&utm_term=%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6&spm=1018.2226.3001.4187">综述—图像处理中的注意力机制_xys430381_1的专栏-CSDN博客_图像注意力机制</a></p><h3 id="2-方法"><a href="#2-方法" class="headerlink" title="2.方法"></a>2.方法</h3><ol><li>空间域（在H*W方向上）（本文采用）</li><li>通道域（在Channel方向上）</li><li>混合域</li></ol><h2 id="三、论文结构"><a href="#三、论文结构" class="headerlink" title="三、论文结构"></a>三、论文结构</h2><p><img src="readme_fig.png" alt="论文结构"></p><h3 id="1-主干网络：Xception"><a href="#1-主干网络：Xception" class="headerlink" title="1.主干网络：Xception"></a>1.主干网络：Xception</h3><p><img src="Xception.png" alt="Xception基本结构"></p><p>Xception是一种主干网络(Backbone)，在许多模型中放置在网络前端，用来提取特征图F。Xception包含三个主要模块：Entry flow, Middle flow 和 Exit flow。</p><ol><li>Entry flow：这一模块包括最开始的两层普通卷积和三个可分离卷积块，其中每个可分离卷积块又包括两次可分离卷积，且第一次可分离卷积需要对Channel数调整，第二次可分离卷积后需要残差块处理。</li><li>Middle flow：这一模块为一个可分离卷积块重复八次，每个卷积快包括三次可分离卷积。</li><li>Exit flow：这一模块包括一个可分离卷积块和后续收尾操作：2*可分离卷积，全局平均池化，全连接等。</li><li>注意力层：论文中提到：“We convert Xception-Net into our model by inserting the attention-based layer between Block 4 and Block 5 of the middle flow, and then fine-tune on DFFD training set.”即注意力层在Middle flow中第4个模块和第5个模块之间。</li></ol><p>代码实现：</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token keyword">class</span> <span class="token class-name">SeparableConv2d</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#可分离卷积(整合了depthwise和pointwose卷积)</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> c_in<span class="token punctuation">,</span> c_out<span class="token punctuation">,</span> ks<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span> dilation<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>SeparableConv2d<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>c <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c_in<span class="token punctuation">,</span> c_in<span class="token punctuation">,</span> ks<span class="token punctuation">,</span> stride<span class="token punctuation">,</span> padding<span class="token punctuation">,</span> dilation<span class="token punctuation">,</span> groups<span class="token operator">=</span>c_in<span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>pointwise <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c_in<span class="token punctuation">,</span> c_out<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span>bias<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>c<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>pointwise<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">class</span> <span class="token class-name">Block</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span><span class="token comment">#基本块</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> c_in<span class="token punctuation">,</span> c_out<span class="token punctuation">,</span> reps<span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token triple-quoted-string string">"""        c_in:输入层channel数        c_out:输出层channel数        reps:一个Block中SeparableConv2d块的数量        stride:卷积步长        start_with_relu:是否为网络中的第一个Block        grow_first：是否为网络中的最后一个Block        """</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Block<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>skip <span class="token operator">=</span> <span class="token boolean">None</span>        self<span class="token punctuation">.</span>skip_bn <span class="token operator">=</span> <span class="token boolean">None</span>        <span class="token keyword">if</span> c_out <span class="token operator">!=</span> c_in <span class="token keyword">or</span> stride <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">:</span><span class="token comment"># 是否加入残差模块的条件判断</span>            self<span class="token punctuation">.</span>skip <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span>c_in<span class="token punctuation">,</span> c_out<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> stride<span class="token operator">=</span>stride<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>            self<span class="token punctuation">.</span>skip_bn <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>c_out<span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        rep <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token punctuation">]</span>        c <span class="token operator">=</span> c_in        <span class="token comment"># 如果是Block中的第一次卷积，就需要调整Channel数</span>        <span class="token keyword">if</span> grow_first<span class="token punctuation">:</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>SeparableConv2d<span class="token punctuation">(</span>c_in<span class="token punctuation">,</span> c_out<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>c_out<span class="token punctuation">)</span><span class="token punctuation">)</span>            c <span class="token operator">=</span> c_out        <span class="token comment"># 中间过程的卷积Channel数不变</span>        <span class="token keyword">for</span> i <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>reps <span class="token operator">-</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">:</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>SeparableConv2d<span class="token punctuation">(</span>c<span class="token punctuation">,</span> c<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>c<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token comment"># 只有Exit flow中卷积块的最后一次卷积，Channel数扩充</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> grow_first<span class="token punctuation">:</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>self<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>SeparableConv2d<span class="token punctuation">(</span>c_in<span class="token punctuation">,</span> c_out<span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span><span class="token punctuation">)</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span>c_out<span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 如果是第一个Block中的第一个卷积块,就需要去掉最开始的relu层，因为已经做过了</span>        <span class="token keyword">if</span> <span class="token keyword">not</span> start_with_relu<span class="token punctuation">:</span>            rep <span class="token operator">=</span> rep<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">:</span><span class="token punctuation">]</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            rep<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        <span class="token keyword">if</span> stride <span class="token operator">!=</span> <span class="token number">1</span><span class="token punctuation">:</span>            rep<span class="token punctuation">.</span>append<span class="token punctuation">(</span>nn<span class="token punctuation">.</span>MaxPool2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>rep <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sequential<span class="token punctuation">(</span><span class="token operator">*</span>rep<span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inp<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>rep<span class="token punctuation">(</span>inp<span class="token punctuation">)</span>        <span class="token keyword">if</span> self<span class="token punctuation">.</span>skip <span class="token keyword">is</span> <span class="token keyword">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>            y <span class="token operator">=</span> self<span class="token punctuation">.</span>skip<span class="token punctuation">(</span>inp<span class="token punctuation">)</span>            y <span class="token operator">=</span> self<span class="token punctuation">.</span>skip_bn<span class="token punctuation">(</span>y<span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            y <span class="token operator">=</span> inp        x <span class="token operator">+=</span> y        <span class="token keyword">return</span> x    <span class="token comment"># 主干网络(注意力层也夹在其中)</span><span class="token keyword">class</span> <span class="token class-name">Xception</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token triple-quoted-string string">"""  Xception optimized for the ImageNet dataset, as specified in  https://arxiv.org/pdf/1610.02357.pdf  """</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> maptype<span class="token punctuation">,</span> templates<span class="token punctuation">,</span> num_classes<span class="token operator">=</span><span class="token number">1000</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>Xception<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>num_classes <span class="token operator">=</span> num_classes                <span class="token comment"># Entry flow</span>        self<span class="token punctuation">.</span>conv1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn1 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>Conv2d<span class="token punctuation">(</span><span class="token number">32</span><span class="token punctuation">,</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn2 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block1 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block2 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">128</span><span class="token punctuation">,</span> <span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block3 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">256</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>                <span class="token comment"># Middle flow</span>        self<span class="token punctuation">.</span>block4 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block5 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block6 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block7 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block8 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block9 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block10 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>block11 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>                <span class="token comment"># Exit flow</span>        self<span class="token punctuation">.</span>block12 <span class="token operator">=</span> Block<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> <span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv3 <span class="token operator">=</span> SeparableConv2d<span class="token punctuation">(</span><span class="token number">1024</span><span class="token punctuation">,</span> <span class="token number">1536</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn3 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">1536</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>conv4 <span class="token operator">=</span> SeparableConv2d<span class="token punctuation">(</span><span class="token number">1536</span><span class="token punctuation">,</span> <span class="token number">2048</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>bn4 <span class="token operator">=</span> nn<span class="token punctuation">.</span>BatchNorm2d<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>last_linear <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">2048</span><span class="token punctuation">,</span> num_classes<span class="token punctuation">)</span>        <span class="token keyword">if</span> maptype <span class="token operator">==</span> <span class="token string">'none'</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span><span class="token builtin">map</span> <span class="token operator">=</span> <span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token boolean">None</span><span class="token punctuation">]</span>        <span class="token keyword">elif</span> maptype <span class="token operator">==</span> <span class="token string">'reg'</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span><span class="token builtin">map</span> <span class="token operator">=</span> RegressionMap<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">)</span>        <span class="token keyword">elif</span> maptype <span class="token operator">==</span> <span class="token string">'tmp'</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span><span class="token builtin">map</span> <span class="token operator">=</span> TemplateMap<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">,</span> templates<span class="token punctuation">)</span>        <span class="token keyword">elif</span> maptype <span class="token operator">==</span> <span class="token string">'pca_tmp'</span><span class="token punctuation">:</span>            self<span class="token punctuation">.</span><span class="token builtin">map</span> <span class="token operator">=</span> PCATemplateMap<span class="token punctuation">(</span><span class="token number">728</span><span class="token punctuation">)</span>        <span class="token keyword">else</span><span class="token punctuation">:</span>            <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">'Unknown map type: `&#123;0&#125;`'</span><span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>maptype<span class="token punctuation">)</span><span class="token punctuation">)</span>            sys<span class="token punctuation">.</span>exit<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">features</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># Entry flow</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv1<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                <span class="token comment"># Middle flow</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block5<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block6<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block7<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># attention-based layer</span>        mask<span class="token punctuation">,</span> vec <span class="token operator">=</span> self<span class="token punctuation">.</span><span class="token builtin">map</span><span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> x <span class="token operator">*</span> mask        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block8<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block9<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block10<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block11<span class="token punctuation">(</span>x<span class="token punctuation">)</span>                <span class="token comment"># Exit flow</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>block12<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn3<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>conv4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>bn4<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> vec    <span class="token keyword">def</span> <span class="token function">logits</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> features<span class="token punctuation">)</span><span class="token punctuation">:</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>features<span class="token punctuation">)</span>        x <span class="token operator">=</span> F<span class="token punctuation">.</span>adaptive_avg_pool2d<span class="token punctuation">(</span>x<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>last_linear<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>        x<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> vec <span class="token operator">=</span> self<span class="token punctuation">.</span>features<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>        x <span class="token operator">=</span> self<span class="token punctuation">.</span>logits<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token keyword">return</span> x<span class="token punctuation">,</span> mask<span class="token punctuation">,</span> vec<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="2-注意力层"><a href="#2-注意力层" class="headerlink" title="2.注意力层"></a>2.注意力层</h3><p><img src="readme_fig.png" alt="论文结构"></p><p>论文中给出了两种计算注意力层的方法，用于相互比较：1. MAM Map    2.Reg.Map。</p><h4 id="1-MAM-Map"><a href="#1-MAM-Map" class="headerlink" title="1. MAM Map"></a>1. MAM Map</h4><p>全称Manipulation Appearance Model，是作者自己提出的一个结构。该结构包括一个可分离卷积块和一个全连接，以此来估计各个区域的权重$\alpha$。此时，$M_{att}=\overline{M}+A·\alpha$，其中$\overline{M}$和$A$通过主成分分析(PCA)提取，将在后文介绍。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># MAM Map</span><span class="token keyword">class</span> <span class="token class-name">TemplateMap</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> c_in<span class="token punctuation">,</span> templates<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>TemplateMap<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>c <span class="token operator">=</span> Block<span class="token punctuation">(</span>c_in<span class="token punctuation">,</span> <span class="token number">364</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> start_with_relu<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">,</span> grow_first<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>l <span class="token operator">=</span> nn<span class="token punctuation">.</span>Linear<span class="token punctuation">(</span><span class="token number">364</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>relu <span class="token operator">=</span> nn<span class="token punctuation">.</span>ReLU<span class="token punctuation">(</span>inplace<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>templates <span class="token operator">=</span> templates    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 可分离卷积</span>        v <span class="token operator">=</span> self<span class="token punctuation">.</span>c<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        v <span class="token operator">=</span> self<span class="token punctuation">.</span>relu<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        <span class="token comment"># 适应性平均池化</span>        v <span class="token operator">=</span> F<span class="token punctuation">.</span>adaptive_avg_pool2d<span class="token punctuation">(</span>v<span class="token punctuation">,</span> <span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        v <span class="token operator">=</span> v<span class="token punctuation">.</span>view<span class="token punctuation">(</span>v<span class="token punctuation">.</span>size<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        <span class="token comment"># 全连接</span>        v <span class="token operator">=</span> self<span class="token punctuation">.</span>l<span class="token punctuation">(</span>v<span class="token punctuation">)</span>        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>v<span class="token punctuation">,</span> self<span class="token punctuation">.</span>templates<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">361</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> mask<span class="token punctuation">,</span> v<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="2-Reg-Map"><a href="#2-Reg-Map" class="headerlink" title="2.Reg. Map"></a>2.Reg. Map</h4><p>这里采用Direct regression，通过一个可分离卷积和$sigmoid$函数可以得到。</p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># Reg. Map</span><span class="token keyword">class</span> <span class="token class-name">RegressionMap</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> c_in<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>RegressionMap<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>c <span class="token operator">=</span> SeparableConv2d<span class="token punctuation">(</span>c_in<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">3</span><span class="token punctuation">,</span> stride<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> padding<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>s <span class="token operator">=</span> nn<span class="token punctuation">.</span>Sigmoid<span class="token punctuation">(</span><span class="token punctuation">)</span>    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token comment"># 可分离卷积</span>        mask <span class="token operator">=</span> self<span class="token punctuation">.</span>c<span class="token punctuation">(</span>x<span class="token punctuation">)</span>        <span class="token comment"># sigmoid</span>        mask <span class="token operator">=</span> self<span class="token punctuation">.</span>s<span class="token punctuation">(</span>mask<span class="token punctuation">)</span>        <span class="token keyword">return</span> mask<span class="token punctuation">,</span> <span class="token boolean">None</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h4 id="3-PCA"><a href="#3-PCA" class="headerlink" title="3.PCA"></a>3.PCA</h4><p>在上文中，$M_{att}=\overline{M}+A·\alpha$，其中$\overline{M}$和$A$通过主成分分析(PCA)提取。其数据来源于FaceApp计算的100个真实的manipulation masks。其提取结果如下(10个主成分)：</p><p><img src="PCA.png" alt="PCA"></p><pre class="line-numbers language-python" data-language="python"><code class="language-python"><span class="token comment"># PCA</span><span class="token keyword">class</span> <span class="token class-name">PCATemplateMap</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token punctuation">:</span>    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> templates<span class="token punctuation">)</span><span class="token punctuation">:</span>        <span class="token builtin">super</span><span class="token punctuation">(</span>PCATemplateMap<span class="token punctuation">,</span> self<span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>        self<span class="token punctuation">.</span>templates <span class="token operator">=</span> templates    <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> x<span class="token punctuation">)</span><span class="token punctuation">:</span>        fe <span class="token operator">=</span> x<span class="token punctuation">.</span>view<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span> <span class="token operator">*</span> x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">3</span><span class="token punctuation">]</span><span class="token punctuation">)</span>        fe <span class="token operator">=</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>fe<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span>        <span class="token comment"># 均值</span>        mu <span class="token operator">=</span> torch<span class="token punctuation">.</span>mean<span class="token punctuation">(</span>fe<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> keepdim<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        fea_diff <span class="token operator">=</span> fe <span class="token operator">-</span> mu        cov_fea <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>fea_diff<span class="token punctuation">,</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>fea_diff<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        B <span class="token operator">=</span> self<span class="token punctuation">.</span>templates<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">361</span><span class="token punctuation">)</span><span class="token punctuation">.</span>repeat<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">)</span>        D <span class="token operator">=</span> torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>bmm<span class="token punctuation">(</span>B<span class="token punctuation">,</span> cov_fea<span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>transpose<span class="token punctuation">(</span>B<span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        <span class="token comment"># 特征值和特征向量</span>        eigen_value<span class="token punctuation">,</span> eigen_vector <span class="token operator">=</span> D<span class="token punctuation">.</span>symeig<span class="token punctuation">(</span>eigenvectors<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>        index <span class="token operator">=</span> torch<span class="token punctuation">.</span>tensor<span class="token punctuation">(</span><span class="token punctuation">[</span><span class="token number">9</span><span class="token punctuation">]</span><span class="token punctuation">)</span><span class="token punctuation">.</span>cuda<span class="token punctuation">(</span><span class="token punctuation">)</span>        eigen <span class="token operator">=</span> torch<span class="token punctuation">.</span>index_select<span class="token punctuation">(</span>eigen_vector<span class="token punctuation">,</span> <span class="token number">2</span><span class="token punctuation">,</span> index<span class="token punctuation">)</span>        v <span class="token operator">=</span> eigen<span class="token punctuation">.</span>squeeze<span class="token punctuation">(</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>        mask <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>v<span class="token punctuation">,</span> self<span class="token punctuation">.</span>templates<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">361</span><span class="token punctuation">)</span><span class="token punctuation">)</span>        mask <span class="token operator">=</span> mask<span class="token punctuation">.</span>reshape<span class="token punctuation">(</span>x<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">1</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">,</span> <span class="token number">19</span><span class="token punctuation">)</span>        <span class="token keyword">return</span> mask<span class="token punctuation">,</span> v<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="3-损失函数"><a href="#3-损失函数" class="headerlink" title="3.损失函数"></a>3.损失函数</h3><p>样本共分为三种场景：supervised, weakly supervised 和 unsupervised。</p><p>但总的损失函数始终为：$\frak L=\frak L_{\rm classifier} +\lambda \ast \frak L_{\rm map}$</p><p>因此，损失函数也分三种情况讨论。</p><p>supervised: $\quad \frak L_{\rm map}=|| \rm M_{\it att}-\rm M_{\it gt}||$</p><p>$\rm M_{\it gt}$是真实图像，用全0矩阵表示。对于完全伪造的图像，则用全1矩阵表示。</p><p>weakly supervised:$\quad \frak L_{\rm map}=\begin{cases}|\rm Sigmoid(M_{\it att})-0|, &amp; \rm if , real    \|\rm max(Sigmoid(M_{\it att}))-0.75|    &amp;\rm if, fake\end{cases}$</p><p>对于真实图像，这种损失可以使注意力机制对其不产生激活。对于伪造图像，其损失会保持足够大。</p><p>unsupervised：$\quad \lambda_m=0$</p><p>此时总损失仅通过分类损失得到。</p><h2 id="四、其他细节"><a href="#四、其他细节" class="headerlink" title="四、其他细节"></a>四、其他细节</h2><h3 id="1-数据集"><a href="#1-数据集" class="headerlink" title="1.数据集"></a>1.数据集</h3><p>论文构造了一个数据集，包括真实图像和虚假图像。</p><p>真实图像主要通过FFHQ和CelebA数据集获得。</p><p>虚假图像的来源有以下几种：</p><ol><li>身份交换和表情修改：FaceBook++</li><li>属性操作：通过StarGAN训练FaceApp的图像得到</li><li>全脸合成：通过与预训练的PGGAN和StyleGAN得到</li></ol><h3 id="2-新的度量标准IINC"><a href="#2-新的度量标准IINC" class="headerlink" title="2.新的度量标准IINC"></a>2.新的度量标准IINC</h3><p>“IINC improves upon other metrics by measuring the non-overlap ratio of both maps, rather than their combined overlap, as in IoU. “即IINC相比于传统的IoU,除了考虑重叠的部分之外，还考虑了非重叠的部分。<br>$$<br>IINC=\frac{1}{3-|\rm U|}\ast \begin{cases}<br>0 &amp;\rm if ; \overline{M_{\it gt}}=0 ;  and ; \overline{M_{\it att}}=0    \<br>1 &amp;\rm if ; \overline{M_{\it gt}}=0 ;  xor ; \overline{M_{\it gt}}=0    \<br>2-\frac{|\rm I|}{\rm M_{\it att}}-\frac{|\rm I|}{\rm M_{\it gt}}    &amp;otherwise<br>\end{cases}<br>$$<br>其中，U和I的含义与IoU中的定义相同，即Union和Intersection。</p><p><img src="IINC.png" alt="IINC"></p><h2 id="五、论文链接"><a href="#五、论文链接" class="headerlink" title="五、论文链接"></a>五、论文链接</h2><p><a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Dang_On_the_Detection_of_Digital_Face_Manipulation_CVPR_2020_paper.pdf">pdf</a></p><p><a href="https://github.com/JStehouwer/FFD_CVPR2020">code</a></p><p><a href="http://cvlab.cse.msu.edu/dffd-dataset.html">dataset</a></p><blockquote><p>2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)</p></blockquote>]]></content>
      
      
      <categories>
          
          <category> CV </category>
          
      </categories>
      
      
        <tags>
            
            <tag> fakeface </tag>
            
            <tag> transformer </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
